{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Train classifier</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Train classifier>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set('spark.local.dir', '/home/jimmy/spark_tmp')\n",
    "conf.set('spark.executor.memory', '15G')\n",
    "conf.set('spark.driver.memory', '15G')\n",
    "conf.set('spark.driver.maxResultSize', '15G')\n",
    "conf.set(\"spark.driver.host\", \"localhost\")\n",
    "#conf.set('spark.cores.max', '8')\n",
    "#conf.set(\"spark.default.parallelism\", 8)\n",
    "sc = SparkContext(appName=\"Train classifier\", conf=conf)\n",
    "sc.setCheckpointDir('checkpoint/')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Train classifier</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f69147dada0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sc.textFile(\"/home/jimmy/Documents/courses/spark/data/*/part-00[0-9]*\")\n",
    "data = spark.read.json(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1977"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vandal = data.filter(data.label == \"vandal\")\n",
    "vandal_count = data.filter(data.label == \"vandal\").count()\n",
    "vandal_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsafe_count = 5_000\n",
    "total_unsafe_count = data.filter(data.label == \"unsafe\").count()\n",
    "unsafe = data.filter(data.label == \"unsafe\").sample(withReplacement = False, fraction= unsafe_count/total_unsafe_count)\n",
    "\n",
    "unsafe_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_count = 10_000\n",
    "total_count = data.count()\n",
    "total_safe_count = total_count - vandal_count - total_unsafe_count\n",
    "safe = data.filter(data.label == \"safe\").sample(withReplacement = False, fraction= safe_count/total_safe_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vandal.union(unsafe).union(safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.write.save(f'/home/jimmy/Documents/courses/spark/notebooks/processed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del vandal\n",
    "del unsafe\n",
    "del safe\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import unified_diff\n",
    "\n",
    "def make_diff(old, new):\n",
    "    additions = []\n",
    "    deletions = []\n",
    "    generator = unified_diff(old.split('\\n'), new.split('\\n'))\n",
    "    for l in generator:\n",
    "        if l.startswith('+'):\n",
    "            additions.append(l[1:])\n",
    "        elif l.startswith('-'):\n",
    "            deletions.append(l[1:])\n",
    "    additions = ' '.join(additions)\n",
    "    deletions = ' '.join(deletions)\n",
    "    return (additions, deletions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, lower\n",
    "from pyspark.sql.types import StringType, LongType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "profanities = [s.strip() for s in open(\"/home/jimmy/Documents/courses/spark/profanities.txt\", \"r\").readlines()]\n",
    "profanities = sc.broadcast(profanities)\n",
    "\n",
    "@udf(\"string\")\n",
    "def additions(old, new):\n",
    "    (additions, _) = make_diff(old, new)\n",
    "    return additions\n",
    "\n",
    "@udf(\"string\")\n",
    "def deletions(old, new):\n",
    "    (_, deletions) = make_diff(old, new)\n",
    "    return deletions\n",
    "\n",
    "@udf(\"long\")\n",
    "def longest_same_character_sequence(additions):\n",
    "    ans, curr = 0, 1\n",
    "    previous = None\n",
    "    for c in additions:\n",
    "        if c == previous:\n",
    "            curr += 1\n",
    "        else:\n",
    "            curr = 1\n",
    "        if curr > ans:\n",
    "            ans = curr\n",
    "        previous = c\n",
    "    return ans\n",
    "\n",
    "@udf(\"long\")\n",
    "def count_profanities(additions):\n",
    "    count = 0\n",
    "    additions = additions\n",
    "    for profanity in profanities.value:\n",
    "        count += additions.count(profanity)\n",
    "    return count\n",
    "\n",
    "drop_list = [\"text_old\", \"text_new\", 'url_page', 'title_page', 'name_user', 'comment']\n",
    "\n",
    "def process_dataframe(df):\n",
    "    return df \\\n",
    "        .withColumn(\"additions\", lower(additions(\"text_old\", \"text_new\"))) \\\n",
    "        .withColumn(\"deletions\", lower(deletions(\"text_old\", \"text_new\"))) \\\n",
    "        .drop(*drop_list) \\\n",
    "        .withColumn(\"profanities\", count_profanities(\"additions\")) \\\n",
    "        .withColumn(\"longest_same_character_sequence\", longest_same_character_sequence(\"additions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- additions: string (nullable = true)\n",
      " |-- deletions: string (nullable = true)\n",
      " |-- profanities: long (nullable = true)\n",
      " |-- longest_same_character_sequence: long (nullable = true)\n",
      "\n",
      "+------+--------------------+--------------------+-----------+-------------------------------+\n",
      "| label|           additions|           deletions|profanities|longest_same_character_sequence|\n",
      "+------+--------------------+--------------------+-----------+-------------------------------+\n",
      "|vandal|++ \n",
      " | name= jona...|-- \n",
      " | name= jona...|          0|                              2|\n",
      "|vandal|++ \n",
      " * bucko is t...|-- \n",
      " * [[bucko (c...|          3|                              3|\n",
      "|vandal|++ \n",
      " the only nat...|-- \n",
      " the only nat...|          3|                              2|\n",
      "|vandal|++ \n",
      " throughout h...|-- \n",
      " throughout h...|          1|                              2|\n",
      "|vandal|++ \n",
      " fan chung wa...|-- \n",
      " fan chung wa...|          2|                              3|\n",
      "|vandal|++ \n",
      "  nf's a bori...|-- \n",
      " david jeffri...|          0|                              2|\n",
      "|vandal|++ \n",
      " | name = kat...|-- \n",
      " | name = kat...|          0|                              2|\n",
      "|vandal|++ \n",
      " {{infobox vi...|-- \n",
      " {{infobox vi...|          0|                              2|\n",
      "|vandal|++ \n",
      " matthew crai...|-- \n",
      " the hebrew n...|          2|                              2|\n",
      "|vandal|++ \n",
      " <!--wikipedi...|-- \n",
      " <!-- do not ...|          0|                              2|\n",
      "|vandal| ++ \n",
      " * i am a t-rex|                -- \n",
      "|          0|                              2|\n",
      "|vandal|++ \n",
      " the series f...|-- \n",
      " the series f...|          0|                              2|\n",
      "|vandal|++ \n",
      " as a separat...|-- \n",
      " as a separat...|          0|                              2|\n",
      "|vandal|++ \n",
      " hello how ar...|-- \n",
      " mainstream s...|          1|                              2|\n",
      "|vandal|++ \n",
      " | ground    ...|-- \n",
      " | ground    ...|          1|                             10|\n",
      "|vandal|++ \n",
      " one of shrev...|-- \n",
      " one of shrev...|          1|                              2|\n",
      "|vandal|++ \n",
      " '''911''' or...|-- \n",
      " '''911''' or...|          0|                              3|\n",
      "|vandal|++ \n",
      " | native_nam...|-- \n",
      " | native_nam...|          1|                              7|\n",
      "|vandal|++ \n",
      " raed abdel j...|-- \n",
      " from the fir...|          0|                              2|\n",
      "|vandal|++ \n",
      " l.m philippi...|                -- \n",
      "|          0|                              2|\n",
      "+------+--------------------+--------------------+-----------+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = process_dataframe(data)\n",
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.save(f'/home/jimmy/Documents/courses/spark/notebooks/processed_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, Word2Vec, StringIndexer, IndexToString, HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import expr, concat, lit\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "text_columns = ['additions', 'deletions']\n",
    "input_columns = text_columns + ['profanities','longest_same_character_sequence']\n",
    "pipelines = {}\n",
    "\n",
    "for col in text_columns:\n",
    "    #regexTokenizer = RegexTokenizer(inputCol=col, outputCol=\"temp1\")\n",
    "    tokenizer = Tokenizer(inputCol=col, outputCol=\"temp1\")\n",
    "    stopwordsRemover = StopWordsRemover(inputCol='temp1', outputCol='temp2')\n",
    "    #word2vec = Word2Vec(inputCol=\"temp2\", outputCol=\"temp3\", numPartitions=16, minCount=10)\n",
    "    #countVectorizer = CountVectorizer(inputCol=\"temp2\", outputCol=\"temp3\")\n",
    "    tf = HashingTF(inputCol=\"temp2\", outputCol=\"temp3\")\n",
    "    idf = IDF(inputCol=\"temp3\", outputCol=\"temp4\")\n",
    "    pipeline = Pipeline(stages=[tokenizer, stopwordsRemover, tf, idf])\n",
    "    pipelines[col] = pipeline.fit(data)\n",
    "    data = pipelines[col].transform(data) \\\n",
    "            .drop(col, 'temp1', 'temp2', 'temp3') \\\n",
    "            .withColumnRenamed('temp4', col)\n",
    "    \n",
    "label_indexer = StringIndexer(inputCol = \"label\", outputCol = \"target\")\n",
    "label_indexer = label_indexer.fit(data)\n",
    "pipelines['label'] = label_indexer\n",
    "data = pipelines['label'].transform(data)\n",
    "\n",
    "data.show(5)\n",
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in text_columns:\n",
    "    pipelines[col].save(f'/home/jimmy/Documents/courses/spark/notebooks/pipeline_{col}')\n",
    "pipelines['label'].save(f'/home/jimmy/Documents/courses/spark/notebooks/pipeline_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=input_columns,outputCol=\"features\")\n",
    "data = assembler.transform(data).drop(*input_columns) #.persist(StorageLevel.DISK_ONLY).collect()\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = vandal_count + unsafe_count + safe_count\n",
    "vandal_ratio = vandal_count / total_count\n",
    "unsafe_ratio = unsafe_count / total_count\n",
    "safe_ratio = safe_count / total_count\n",
    "(vandal_ratio, unsafe_ratio, safe_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = data.withColumn('weight', F.when(F.col('label')=='safe', safe_ratio).when(F.col('label')=='unsafe', unsafe_ratio).otherwise(vandal_ratio))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.repartition(10).write.save(f'/home/jimmy/Documents/courses/spark/notebooks/processed_data_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.load.read(f'/home/jimmy/Documents/courses/spark/notebooks/processed_data_2')\n",
    "from pyspark.ml.feature import StringIndexerModel, IndexToString\n",
    "pipelines = {}\n",
    "pipelines[\"label\"] = StringIndexerModel.load(f'/home/jimmy/Documents/courses/spark/notebooks/pipeline_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "scaler = scaler.fit(data)\n",
    "data = scaler.transform(data).drop(\"features\").withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3], seed = 100)\n",
    "from pyspark import StorageLevel\n",
    "trainingData.persist(StorageLevel.MEMORY_ONLY)\n",
    "testData.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol='target', weightCol=\"weight\", maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=pipelines['label'].labels)\n",
    "\n",
    "pipeline = Pipeline(stages=[lr, label_converter])\n",
    "\n",
    "results = pipeline.fit(trainingData).transform(testData)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\")\n",
    "\n",
    "evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator # Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build()) # Create 10-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=10)\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "predictions = cvModel.transform(testData)\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.save(f'/home/jimmy/Documents/courses/spark/notebooks/logistic_regression_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[cvModel, label_converter])\n",
    "pipeline.save(f'/home/jimmy/Documents/courses/spark/notebooks/pipeline_logistic_regression_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
