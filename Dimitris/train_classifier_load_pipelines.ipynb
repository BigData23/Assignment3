{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Train classifier</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Train classifier>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "conf = conf.set('spark.local.dir', '/home/jimmy/spark_tmp')\n",
    "conf = conf.set('spark.executor.memory', '15G')\n",
    "conf = conf.set('spark.driver.memory', '15G')\n",
    "conf = conf.set('spark.driver.maxResultSize', '15G')\n",
    "conf = conf.set(\"spark.driver.host\", \"localhost\")\n",
    "#conf.set('spark.cores.max', '8')\n",
    "#conf = conf.set(\"spark.default.parallelism\", 8)\n",
    "sc = SparkContext(appName=\"Train classifier\", conf=conf)\n",
    "sc.setCheckpointDir('checkpoint/')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Train classifier</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f369c348518>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sc.textFile(\"/home/jimmy/Documents/courses/spark/data/*/part-00[0-9]*\")\n",
    "data = spark.read.json(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "#data = data.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#data = data.checkpoint(eager=True)\n",
    "\n",
    "#data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vandal = data.filter(data.label == \"vandal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsafe = data.filter(data.label == \"unsafe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe = data.filter(data.label == \"safe\").limit(60_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vandal.union(unsafe).union(safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#data = data.checkpoint(eager=True)\n",
    "#data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.load(\"/home/jimmy/Documents/courses/spark/notebooks/processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import unified_diff\n",
    "\n",
    "def make_diff(old, new):\n",
    "    additions = []\n",
    "    deletions = []\n",
    "    generator = unified_diff(old.split(), new.split())\n",
    "    for l in generator:\n",
    "        if l.startswith('+'):\n",
    "            additions.append(l[1:])\n",
    "        elif l.startswith('-'):\n",
    "            deletions.append(l[1:])\n",
    "    additions = ' '.join(additions)\n",
    "    deletions = ' '.join(deletions)\n",
    "    return (additions, deletions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, lower\n",
    "from pyspark.sql.types import StringType, LongType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "profanities = [s.strip() for s in open(\"/home/jimmy/Documents/courses/spark/profanities.txt\", \"r\").readlines()]\n",
    "profanities\n",
    "\n",
    "@udf(\"string\")\n",
    "def additions(old, new):\n",
    "    (additions, _) = make_diff(old, new)\n",
    "    return additions\n",
    "\n",
    "@udf(\"string\")\n",
    "def deletions(old, new):\n",
    "    (_, deletions) = make_diff(old, new)\n",
    "    return deletions\n",
    "\n",
    "@udf(\"long\")\n",
    "def longest_same_character_sequence(additions):\n",
    "    ans, curr = 0, 1\n",
    "    previous = None\n",
    "    for c in additions:\n",
    "        if c == previous:\n",
    "            curr += 1\n",
    "        else:\n",
    "            curr = 1\n",
    "        if curr > ans:\n",
    "            ans = curr\n",
    "        previous = c\n",
    "    return ans\n",
    "\n",
    "@udf(\"long\")\n",
    "def count_profanities(additions):\n",
    "    count = 0\n",
    "    additions = additions\n",
    "    for profanity in profanities:\n",
    "        count += additions.count(profanity)\n",
    "    return count\n",
    "\n",
    "# count_profanities_udf = udf(count_profanities, LongType())\n",
    "\n",
    "def process_dataframe(df):\n",
    "    return df \\\n",
    "        .withColumn(\"additions\", lower(additions(\"text_old\", \"text_new\"))) \\\n",
    "        .withColumn(\"deletions\", lower(deletions(\"text_old\", \"text_new\"))) \\\n",
    "        .drop(\"text_old\") \\\n",
    "        .withColumn(\"profanities\", count_profanities(\"additions\")) \\\n",
    "        .withColumn(\"longest_same_character_sequence\", longest_same_character_sequence(\"additions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+-------------------------------+\n",
      "|             comment|label|       name_user|            text_new|          title_page|            url_page|           additions|           deletions|profanities|longest_same_character_sequence|\n",
      "+--------------------+-----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+-------------------------------+\n",
      "|                    | safe|  Jellysandwich0|{{short descripti...|A Complicated Kin...|//en.wikipedia.or...|          ++ \n",
      " marry| -- \n",
      " get married to|          0|                              2|\n",
      "|→‎Bio oil:Updated...| safe|    Mirja kemppi|{{short descripti...|       UPM (company)|//en.wikipedia.or...|                ++ \n",
      "|-- \n",
      " ===bio oil==...|          0|                              2|\n",
      "|       →‎Taxonomy:ce| safe|     Rogermccart|{{speciesbox\n",
      "| na...|  Western wattlebird|//en.wikipedia.or...|++ \n",
      " collection\"....|-- \n",
      " collection\";...|          0|                              2|\n",
      "|                    | safe|Itzhak Rosenberg|{{Tone|date=April...|         Samara flag|//en.wikipedia.or...| ++ \n",
      " 3<sup>rd</sup>|          -- \n",
      " third|          0|                              2|\n",
      "|        poor Spanish| safe|  Jellysandwich0|{{Infobox televis...|          Toda mujer|//en.wikipedia.or...|++ \n",
      " marry pérez ...|-- \n",
      " get married ...|          0|                              2|\n",
      "|Cleaned up usingA...| safe|       Keith-264|{{Use dmy dates|d...|      Charles Noguès|//en.wikipedia.or...|++ \n",
      " gilson|title...|-- \n",
      " gilson|url=h...|          1|                              2|\n",
      "|→‎Classification:...| safe|        Rjwilmsi|{{More citations ...|Hereditary inclus...|//en.wikipedia.or...|++ \n",
      " name=carrill...|-- \n",
      " name=carrill...|          1|                              2|\n",
      "|General fixesand ...| safe|      GoingBatty|{{other people|De...|       Dennis Miller|//en.wikipedia.or...|++ \n",
      " 2000|first=c...|-- \n",
      " 2000|author=...|          9|                              4|\n",
      "|          →‎Planning| safe|        3family6|{{short descripti...|American logistic...|//en.wikipedia.or...|       ++ \n",
      " supplies|       -- \n",
      " supplied|          0|                              2|\n",
      "|       Moved content| safe|          P3Y229|{{Use American En...|Withdrawal of U.S...|//en.wikipedia.or...|++ \n",
      " in september...|-- \n",
      " in september...|          1|                              3|\n",
      "|→‎Cause of the crash| safe| Александр Мотин|{{short descripti...|Malaysia Airlines...|//en.wikipedia.or...|              ++ \n",
      " a|              -- \n",
      " a|          0|                              2|\n",
      "|clean up,typo(s) ...| safe|      GoingBatty|{{About|the actre...|    Jennifer O'Neill|//en.wikipedia.or...|++ \n",
      " ex-husband |...|-- \n",
      " ex husband |...|          0|                              2|\n",
      "|Minor grammatical...| safe|        Ninja999|{{Infobox religio...|    Shwedagon Pagoda|//en.wikipedia.or...|             ++ \n",
      " to|                -- \n",
      "|          0|                              2|\n",
      "|→‎Mixed doubles:o...| safe|Allthegoldmedals|{{short descripti...|        Rachel Homan|//en.wikipedia.or...|++ \n",
      " e. harnden john|-- \n",
      " [[e. harnden...|          0|                              2|\n",
      "|                    | safe|       Mztourist|{{Infobox militar...|  Operation Kentucky|//en.wikipedia.or...|++ \n",
      " [[1st divisi...|-- \n",
      " units from [...|          0|                              2|\n",
      "|               units| safe|    Marc Lacoste|{{Use American En...|Scaled Composites...|//en.wikipedia.or...|++ \n",
      " {{convert|32...|-- \n",
      " {{convert|32...|          0|                              5|\n",
      "|Changed episode n...| safe|    DonRobodroid|{{Infobox Televis...|Islanded in a Str...|//en.wikipedia.or...|++ \n",
      " 18 | editor ...|   -- \n",
      " 20 twentieth|          0|                              2|\n",
      "|            clean up| safe|    CAPTAIN RAJU|'''Gonzalo Casals...|      Gonzalo Casals|//en.wikipedia.or...|++ \n",
      " '''gonzalo c...|-- \n",
      " = gonzalo ca...|          0|                              3|\n",
      "|                    | safe|       Markussep|{{Infobox German ...|        Schwarmstedt|//en.wikipedia.or...|                ++ \n",
      "|-- \n",
      " |population_...|          0|                              2|\n",
      "|clean up,typo(s) ...| safe|    CAPTAIN RAJU|{{Use dmy dates|d...|Markandeya River,...|//en.wikipedia.or...|++ \n",
      " river]</ref>...|-- \n",
      " river]</ref>...|          0|                              3|\n",
      "+--------------------+-----+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = process_dataframe(data)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#data = data.checkpoint(eager=True)\n",
    "#data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+-----------+-------------------------------+\n",
      "|label|            text_new|           additions|           deletions|profanities|longest_same_character_sequence|\n",
      "+-----+--------------------+--------------------+--------------------+-----------+-------------------------------+\n",
      "| safe|{{short descripti...|          ++ \n",
      " marry| -- \n",
      " get married to|          0|                              2|\n",
      "| safe|{{short descripti...|                ++ \n",
      "|-- \n",
      " ===bio oil==...|          0|                              2|\n",
      "| safe|{{speciesbox\n",
      "| na...|++ \n",
      " collection\"....|-- \n",
      " collection\";...|          0|                              2|\n",
      "| safe|{{Tone|date=April...| ++ \n",
      " 3<sup>rd</sup>|          -- \n",
      " third|          0|                              2|\n",
      "| safe|{{Infobox televis...|++ \n",
      " marry pérez ...|-- \n",
      " get married ...|          0|                              2|\n",
      "+-----+--------------------+--------------------+--------------------+-----------+-------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_list = ['url_page', 'title_page', 'name_user', 'comment']\n",
    "data = data.drop(*drop_list) #.coalesce(10_000).cache()\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#data = data.checkpoint(eager=True)\n",
    "#data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexerModel, IndexToString\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "text_columns = ['text_new', 'additions', 'deletions']\n",
    "input_columns = text_columns + ['profanities','longest_same_character_sequence']\n",
    "pipelines = {}\n",
    "\n",
    "for col in text_columns:\n",
    "    pipelines[col] = PipelineModel.load(f'/home/jimmy/Documents/courses/spark/notebooks/pipeline_{col}')\n",
    "pipelines['label'] = StringIndexerModel.load(f'/home/jimmy/Documents/courses/spark/notebooks/pipeline_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in text_columns:\n",
    "    data = pipelines[col].transform(data).drop(col, 'temp1', 'temp2', 'temp3').withColumnRenamed('temp4', col)\n",
    "data = pipelines['label'].transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#data = data.checkpoint(eager=True)\n",
    "#data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------------+\n",
      "|label|target|            features|\n",
      "+-----+------+--------------------+\n",
      "| safe|   0.0|(786434,[373,400,...|\n",
      "| safe|   0.0|(786434,[67,227,4...|\n",
      "| safe|   0.0|(786434,[1707,195...|\n",
      "| safe|   0.0|(786434,[3484,357...|\n",
      "| safe|   0.0|(786434,[285,315,...|\n",
      "+-----+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=input_columns,outputCol=\"features\")\n",
    "data = assembler.transform(data).drop(*input_columns)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#data = data.checkpoint(eager=True)\n",
    "#data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.persist(StorageLevel.DISK_ONLY)\n",
    "#data = data.checkpoint(eager=True)\n",
    "#data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 42029\n",
      "Test Dataset Count: 17910\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3], seed = 100)\n",
    "trainingData = trainingData.persist(StorageLevel.MEMORY_ONLY)\n",
    "testData = testData.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainingData = trainingData.persist(StorageLevel.DISK_ONLY)\n",
    "#trainingData = trainingData.checkpoint(eager=True)\n",
    "#trainingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testData = testData.persist(StorageLevel.DISK_ONLY)\n",
    "#testData = testData.checkpoint(eager=True)\n",
    "#testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol='target', maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=pipelines['label'].labels)\n",
    "\n",
    "pipeline = Pipeline(stages=[lr, label_converter])\n",
    "pipelineModel = pipeline.fit(trainingData)\n",
    "\n",
    "results = pipelineModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------------+--------------------+--------------------+----------+--------------+\n",
      "|label|target|            features|       rawPrediction|         probability|prediction|predictedLabel|\n",
      "+-----+------+--------------------+--------------------+--------------------+----------+--------------+\n",
      "| safe|   0.0|(786434,[1,55,60,...|[2.11138391163214...|[0.40368446211901...|       1.0|        unsafe|\n",
      "| safe|   0.0|(786434,[1,267,33...|[6.03109964896998...|[0.99857887952331...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[1,297,30...|[0.24515096567145...|[0.07113210577865...|       1.0|        unsafe|\n",
      "| safe|   0.0|(786434,[1,347,21...|[1.57948638393767...|[0.59916883116242...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[2,844,10...|[1.02830647554900...|[0.51674961481456...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[3,46,83,...|[3.36504599293499...|[0.97713290003446...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[3,153,37...|[1.37325166877675...|[0.61838320197259...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[3,285,10...|[1.54174518103247...|[0.73333143853511...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[4,14,18,...|[-0.2752221360371...|[0.04011997384544...|       1.0|        unsafe|\n",
      "| safe|   0.0|(786434,[4,14,20,...|[3.34742046852898...|[0.89796178969458...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[4,198,52...|[-0.7918191211170...|[0.02386182368607...|       1.0|        unsafe|\n",
      "| safe|   0.0|(786434,[4,223,48...|[1.75191251771997...|[0.80512506590913...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[4,285,50...|[2.46122623748806...|[0.88070226169128...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[5,14,53,...|[1.14648211545918...|[0.46141757291708...|       1.0|        unsafe|\n",
      "| safe|   0.0|(786434,[5,157,21...|[0.29669439066455...|[0.03252443780248...|       1.0|        unsafe|\n",
      "| safe|   0.0|(786434,[5,571,12...|[1.47811137146562...|[0.69145844072048...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[6,58,202...|[2.10642493460409...|[0.65119515918324...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[7,14,55,...|[2.82005671241508...|[0.50171348722304...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[7,2571,3...|[1.53521047575616...|[0.72108123510917...|       0.0|          safe|\n",
      "| safe|   0.0|(786434,[8,14,26,...|[6.56187935950767...|[0.99941968160396...|       0.0|          safe|\n",
      "+-----+------+--------------------+--------------------+--------------------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- target: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- predictedLabel: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\")\n",
    "\n",
    "evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel.save(f'/home/jimmy/Documents/courses/spark/notebooks/pipeline_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator # Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build()) # Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "predictions = cvModel.transform(testData)\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.save(f'/home/jimmy/Documents/courses/spark/notebooks/logistic_regression_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[cvModel, label_converter])\n",
    "pipeline.save(f'/home/jimmy/Documents/courses/spark/notebooks/pipeline_logistic_regression_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
